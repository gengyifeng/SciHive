diff -urN src_origin/build.properties src/build.properties
--- src_origin/build.properties	2013-04-12 12:31:20.283528416 +0800
+++ src/build.properties	2013-04-10 14:51:14.905770870 +0800
@@ -27,14 +27,16 @@
 javac.args.warnings=
 
 hadoop-0.20.version=0.20.2
-hadoop-0.20S.version=1.0.0
-hadoop-0.23.version=2.0.0-alpha
+#hadoop-0.20.version=1.0.4
+hadoop-0.20S.version=1.0.4
+hadoop-0.23.version=2.0.3-alpha
 hadoop.version=${hadoop-0.20.version}
 hadoop.security.version=${hadoop-0.20S.version}
 # Used to determine which set of Hadoop artifacts we depend on.
 # - 20: hadoop-core, hadoop-test
 # - 23: hadoop-common, hadoop-mapreduce-*, etc
-hadoop.mr.rev=20
+#hadoop.mr.rev=20
+hadoop.mr.rev=20S
 
 build.dir.hive=${hive.root}/build
 build.dir.hadoop=${build.dir.hive}/hadoopcore
diff -urN src_origin/cmd src/cmd
--- src_origin/cmd	1970-01-01 08:00:00.000000000 +0800
+++ src/cmd	2013-04-10 14:48:49.449763830 +0800
@@ -0,0 +1,2 @@
+export ANT_OPTS="-Dhttp.proxyHost=127.0.0.1 -Dhttp.proxyPort=8087"
+ant package -Dhadoop.version=1.0.4 -Dhadoop-0.20S.version=1.0.4 -Dhadoop.mr.rev=20S
diff -urN src_origin/common/ivy.xml src/common/ivy.xml
--- src_origin/common/ivy.xml	2013-04-12 12:31:20.071528408 +0800
+++ src/common/ivy.xml	2013-04-10 14:57:43.817789692 +0800
@@ -80,8 +80,32 @@
       <exclude org="commons-daemon" module="commons-daemon"/><!--bad POM-->
       <exclude org="org.apache.commons" module="commons-daemon"/><!--bad POM-->
     </dependency>
+    
+    <dependency org="org.apache.hadoop" name="hadoop-core"
+                rev="${hadoop-0.20S.version}"
+                conf="hadoop20S.compile->default" transitive="false">
+      <include type="jar"/>
+      <exclude org="commons-daemon" module="commons-daemon"/><!--bad POM-->
+      <exclude org="org.apache.commons" module="commons-daemon"/><!--bad POM-->
+    </dependency>
+
+    <dependency org="org.apache.hadoop" name="hadoop-test"
+                rev="${hadoop-0.20S.version}"
+                conf="hadoop20S.compile->default" transitive="false">
+      <include type="jar"/>
+      <exclude org="commons-daemon" module="commons-daemon"/><!--bad POM-->
+      <exclude org="org.apache.commons" module="commons-daemon"/><!--bad POM-->
+    </dependency>
+    <dependency org="org.apache.hadoop" name="hadoop-tools"
+                rev="${hadoop-0.20S.version}"
+                conf="hadoop20S.compile->default" transitive="false">
+      <include type="jar"/>
+      <exclude org="commons-daemon" module="commons-daemon"/><!--bad POM-->
+      <exclude org="org.apache.commons" module="commons-daemon"/><!--bad POM-->
+    </dependency>
+    <dependency org="javax.ws.rs" name="jsr311-api" rev="${jsr311.version}" conf="hadoop20S.test->default" />
 
-    <dependency org="org.apache.hive" name="hive-shims" rev="${version}"
+<dependency org="org.apache.hive" name="hive-shims" rev="${version}"
                 conf="compile->default" transitive="false" />
     <dependency org="commons-cli" name="commons-cli" rev="${commons-cli.version}"/>
     <dependency org="org.apache.commons" name="commons-compress" rev="${commons-compress.version}"/>
diff -urN src_origin/contrib/.classpath src/contrib/.classpath
--- src_origin/contrib/.classpath	1970-01-01 08:00:00.000000000 +0800
+++ src/contrib/.classpath	2013-04-17 20:59:32.604018676 +0800
@@ -0,0 +1,10 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<classpath>
+	<classpathentry kind="src" path="src/java"/>
+	<classpathentry kind="src" path="src/test"/>
+	<classpathentry kind="con" path="org.eclipse.jdt.launching.JRE_CONTAINER"/>
+	<classpathentry kind="lib" path="/ncSrc-4.2/cdm/target/netcdfAll-4.2.jar"/>
+	<classpathentry kind="lib" path="/home/yifeng/hbase-0.94.6.1/hbase-0.94.6.1.jar"/>
+	<classpathentry kind="lib" path="/home/yifeng/hive-0.10.0/src/build/ql/hive-exec-0.10.0-SNAPSHOT.jar"/>
+	<classpathentry kind="output" path="bin"/>
+</classpath>
diff -urN src_origin/contrib/.project src/contrib/.project
--- src_origin/contrib/.project	1970-01-01 08:00:00.000000000 +0800
+++ src/contrib/.project	2013-04-17 20:52:58.395999597 +0800
@@ -0,0 +1,17 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<projectDescription>
+	<name>hive_contrib</name>
+	<comment></comment>
+	<projects>
+	</projects>
+	<buildSpec>
+		<buildCommand>
+			<name>org.eclipse.jdt.core.javabuilder</name>
+			<arguments>
+			</arguments>
+		</buildCommand>
+	</buildSpec>
+	<natures>
+		<nature>org.eclipse.jdt.core.javanature</nature>
+	</natures>
+</projectDescription>
diff -urN src_origin/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/NcFileInputFormat.java src/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/NcFileInputFormat.java
--- src_origin/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/NcFileInputFormat.java	1970-01-01 08:00:00.000000000 +0800
+++ src/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/NcFileInputFormat.java	2014-01-06 00:43:03.095044265 +0800
@@ -0,0 +1,751 @@
+package org.apache.hadoop.hive.contrib.fileformat.netcdf;
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.List;
+import org.apache.commons.logging.Log;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.BlockLocation;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.io.InputFormatChecker;
+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
+import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrLessThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPLessThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr;
+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputSplit;
+import org.apache.hadoop.mapred.InvalidInputException;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.net.NetworkTopology;
+import ucar.ma2.Array;
+import ucar.ma2.Index;
+import ucar.ma2.InvalidRangeException;
+import ucar.ma2.Section;
+import ucar.nc2.Dimension;
+import ucar.nc2.NetcdfFile;
+import ucar.nc2.Variable;
+
+public class NcFileInputFormat extends FileInputFormat<LongWritable, Text>
+  implements InputFormatChecker
+{
+  private static final double SPLIT_SLOP = 1.1;
+  private  long minSplitSize = 1;
+  private static final PathFilter hiddenFileFilter = new PathFilter() {
+    public boolean accept(Path p) {
+      String name = p.getName();
+      return (!name.startsWith("_")) && (!name.startsWith("."));
+    }
+  };
+
+  protected boolean isSplitable(FileSystem fs, Path filename)
+  {
+    return false;
+  }
+
+  public RecordReader<LongWritable, Text> getRecordReader(InputSplit split, JobConf conf, Reporter reporter)
+    throws IOException
+  {
+    reporter.setStatus(split.toString());
+    return new NcFileRecordReader(conf, split);
+  }
+
+  protected FileStatus[] listStatus(JobConf job, FileStatus file) throws IOException
+  {
+    Path p = file.getPath();
+
+    List result = new ArrayList();
+    List errors = new ArrayList();
+
+    List filters = new ArrayList();
+    filters.add(hiddenFileFilter);
+    PathFilter jobFilter = getInputPathFilter(job);
+    if (jobFilter != null) {
+      filters.add(jobFilter);
+    }
+    PathFilter inputFilter = new MultiPathFilter(filters);
+    FileSystem fs = p.getFileSystem(job);
+    FileStatus[] matches = fs.globStatus(p, inputFilter);
+    if (matches == null)
+      errors.add(new IOException("Input path does not exist: " + p));
+    else if (matches.length == 0) {
+      errors.add(new IOException("Input Pattern " + p + " matches 0 files"));
+    }
+    else {
+      for (FileStatus globStat : matches) {
+        if (globStat.isDir())
+          for (FileStatus stat : fs.listStatus(globStat.getPath(), inputFilter))
+          {
+            result.add(stat);
+          }
+        else {
+          result.add(globStat);
+        }
+      }
+    }
+    if (!errors.isEmpty()) {
+      throw new InvalidInputException(errors);
+    }
+    return (FileStatus[])result.toArray(new FileStatus[result.size()]);
+  }
+
+  public long getLen(JobConf job, FileStatus file)
+    throws IOException
+  {
+    long all = 0L;
+    if (file.isDir()) {
+      FileStatus[] files = listStatus(job, file);
+      for (FileStatus subfile : files)
+        all += getLen(job, subfile);
+    }
+    else {
+      all += file.getLen();
+    }
+    return all;
+  }
+
+  public ArrayList<FileSplit> getSplits(JobConf job, FileStatus dirFile, long totalSize, int numSplits)
+    throws IOException
+  {
+    ArrayList fsArr = new ArrayList(numSplits);
+    FileStatus[] files = listStatus(job, dirFile);
+    long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
+    long minSize = Math.max(job.getLong("mapred.min.split.size", 1), 1);
+
+    NetworkTopology clusterMap = new NetworkTopology();
+    for (FileStatus file : files) {
+      if (!file.isDir()) {
+        Path path = file.getPath();
+        FileSystem fs = path.getFileSystem(job);
+        long length = file.getLen();
+        BlockLocation[] blkLocations = fs.getFileBlockLocations(file, 0L, length);
+
+        if ((length != 0L) && (isSplitable(fs, path))) {
+          long blockSize = file.getBlockSize();
+          long splitSize = computeSplitSize(goalSize, minSize, blockSize);
+
+          long bytesRemaining = length;
+          while (bytesRemaining / splitSize > 1.1) {
+            String[] splitHosts = getSplitHosts(blkLocations, length - bytesRemaining, splitSize, clusterMap);
+
+            fsArr.add(new FileSplit(path, length - bytesRemaining, splitSize, splitHosts));
+
+            bytesRemaining -= splitSize;
+          }
+
+          if (bytesRemaining != 0L) {
+            fsArr.add(new FileSplit(path, length - bytesRemaining, bytesRemaining, blkLocations[(blkLocations.length - 1)].getHosts()));
+          }
+
+        }
+        else if (length != 0L) {
+          String[] splitHosts = getSplitHosts(blkLocations, 0L, length, clusterMap);
+
+          fsArr.add(new FileSplit(path, 0L, length, splitHosts));
+        }
+        else {
+          fsArr.add(new FileSplit(path, 0L, length, new String[0]));
+        }
+      } else {
+        fsArr.addAll(getSplits(job, file, totalSize, numSplits));
+      }
+
+    }
+
+    return fsArr;
+  }
+
+  public InputSplit[] getSplits(JobConf job, int numSplits)
+    throws IOException
+  {
+    FileStatus[] files = listStatus(job);
+
+    long totalSize = 0L;
+    for (FileStatus file : files) {
+      if (file.isDir()) {
+        totalSize += getLen(job, file);
+      }
+      totalSize += file.getLen();
+    }
+
+    long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
+    long minSize = Math.max(job.getLong("mapred.min.split.size", 1), 1);
+
+    ArrayList splits = new ArrayList(numSplits);
+    NetworkTopology clusterMap = new NetworkTopology();
+    for (FileStatus file : files) {
+      if (!file.isDir()) {
+        Path path = file.getPath();
+        FileSystem fs = path.getFileSystem(job);
+        long length = file.getLen();
+        BlockLocation[] blkLocations = fs.getFileBlockLocations(file, 0L, length);
+
+        if ((length != 0L) && (isSplitable(fs, path))) {
+          long blockSize = file.getBlockSize();
+          long splitSize = computeSplitSize(goalSize, minSize, blockSize);
+
+          long bytesRemaining = length;
+          while (bytesRemaining / splitSize > 1.1D) {
+            String[] splitHosts = getSplitHosts(blkLocations, length - bytesRemaining, splitSize, clusterMap);
+
+            splits.add(new FileSplit(path, length - bytesRemaining, splitSize, splitHosts));
+
+            bytesRemaining -= splitSize;
+          }
+
+          if (bytesRemaining != 0L) {
+            splits.add(new FileSplit(path, length - bytesRemaining, bytesRemaining, blkLocations[(blkLocations.length - 1)].getHosts()));
+          }
+
+        }
+        else if (length != 0L) {
+          String[] splitHosts = getSplitHosts(blkLocations, 0L, length, clusterMap);
+
+          splits.add(new FileSplit(path, 0L, length, splitHosts));
+        }
+        else {
+          splits.add(new FileSplit(path, 0L, length, new String[0]));
+        }
+      } else {
+        splits.addAll(getSplits(job, file, totalSize, numSplits));
+      }
+    }
+    LOG.debug("Total # of splits: " + splits.size());
+    return (InputSplit[])splits.toArray(new FileSplit[splits.size()]);
+  }
+
+  public boolean validateInput(FileSystem fs, HiveConf conf, ArrayList<FileStatus> files)
+    throws IOException
+  {
+    if (files.size() <= 0) {
+      return false;
+    }
+    for (int fileId = 0; fileId < files.size(); fileId++) {
+      try {
+        NetcdfFile ncfile = NetcdfFile.open(((FileStatus)files.get(fileId)).getPath().toString());
+        ncfile.close();
+      } catch (IOException e) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  private static class MultiPathFilter
+    implements PathFilter
+  {
+    private final List<PathFilter> filters;
+
+    public MultiPathFilter(List<PathFilter> filters)
+    {
+      this.filters = filters;
+    }
+
+    public boolean accept(Path path) {
+      for (PathFilter filter : this.filters) {
+        if (!filter.accept(path)) {
+          return false;
+        }
+      }
+      return true;
+    }
+  }
+
+  static class NcFileRecordReader
+    implements RecordReader<LongWritable, Text>
+  {
+    private final long start;
+    private final long end;
+    private NetcdfFile ncfile = null;
+    private final String filename;
+    private int count=0;
+    private Index idx = null;
+    private final String seprator;
+    private final ArrayList<Array> varArr;
+    private final ArrayList<Variable> vars;
+    private final ArrayList<Array> dimArr;
+    int[] dimMap;
+    int[] types;
+    int[] shrinkedTypes;
+    int validIDsCount;
+    int[] shape;
+    int[] validShape;
+    int[] shapeDivider;
+    int[] validShapeDivider;
+    int totalSize=0;
+    int dimsSize=0;
+    public HashMap<String,Double> equalValueMap= new HashMap();
+    public HashMap<String, Double> greaterValueMap = new HashMap();
+    public HashMap<String, Boolean> greaterBoolMap = new HashMap();
+    public HashMap<String, Double> lessValueMap = new HashMap();
+    public HashMap<String, Boolean> lessBoolMap = new HashMap();
+    public HashMap<String, ArrayList<Double>> notEqualMap = new HashMap();
+    public HashSet<String> fixs = new HashSet();
+    public HashSet<String> cols = new HashSet();
+    public HashMap<String,ArrayList<Integer>> validIndexes=new HashMap();
+    public ArrayList<ArrayList<Integer>> validIndexArr;
+    boolean noResult=false;
+    boolean notEqual(Double input,Double num){
+        return !input.equals(num)?true:false;
+    }
+    boolean greaterEqual(Double input,Double num){
+        return input>=num?true:false;
+    }
+    boolean greater(Double input,Double num){
+        return input>num?true:false;
+    }
+    boolean lessEqual(Double input,Double num){
+        return input<=num?true:false;
+    }
+    boolean less(Double input,Double num){
+        return input<num?true:false;
+    }
+    boolean eval_value(Double val,String col,Double gValue,Double lValue,ArrayList<Double> nValues){
+        if(gValue!=null){
+            if(greaterBoolMap.get(col)==true){
+                if(!greaterEqual(val,gValue)){
+                    return false;
+                }
+            }else{
+                if(!greater(val,gValue)){
+                    return false;
+                }
+            }
+        }
+        if(lValue!=null){
+            if(lessBoolMap.get(col)==true){
+                if(!lessEqual(val,lValue)){
+                    return false;
+                }
+            }else{
+                if(!less(val,lValue)){
+                    return false;
+                }
+            }
+        }
+        if(nValues!=null){
+            for(Double d:nValues){
+                if(!notEqual(val,d)){
+                    return false;
+                }
+            }
+        }
+        return true;
+    }
+    void evalDimensionVariable(Variable v,String col) throws IOException{
+        if(v.getDimensions().size()>1)
+            return;
+        Array arr=v.read();
+        long size=arr.getSize();
+        ArrayList<Integer> validIndex=new ArrayList<Integer>();
+        Double gValue=greaterValueMap.get(col);
+        Double lValue=lessValueMap.get(col);
+        ArrayList<Double> nValues=notEqualMap.get(col);
+        Double val=null;
+        for(int i=0;i<size;i++){
+            val=new Double(arr.getObject(i).toString());
+            if(!eval_value(val,col,gValue,lValue,nValues)){
+                continue;
+            }
+            validIndex.add(i);    
+        }
+        if(validIndex.size()==0)
+            noResult=true;
+        validIndexes.put(col,validIndex);
+    }
+    void updateGreaterValueMap(String col,Double value,Boolean equal){
+        Double old=greaterValueMap.get(col);
+        if(old==null){
+            this.greaterValueMap.put(col, value);
+            this.greaterBoolMap.put(col, equal);
+        }else{
+            if(value>old){
+                this.greaterValueMap.put(col, value);
+                this.greaterBoolMap.put(col, equal);
+            }else if(value.equals(old)){
+                if(!equal)
+                    this.greaterBoolMap.put(col,equal);
+            }
+        }
+    }
+    void updateLessValueMap(String col,Double value,Boolean equal){
+        Double old=lessValueMap.get(col);
+        if(old==null){
+            this.lessValueMap.put(col, value);
+            this.lessBoolMap.put(col, equal);
+        }else{
+            if(value<old){
+                this.lessValueMap.put(col, value);
+                this.lessBoolMap.put(col, equal);
+            }else if(value.equals(old)){
+                if(!equal)
+                    this.lessBoolMap.put(col,equal);
+            }
+        }
+    }
+    public void updateConditions(String col, Double value, GenericUDFBaseCompare op, boolean rightOrder) {
+      if ((op instanceof GenericUDFOPEqual)) {
+        updateGreaterValueMap(col,value,Boolean.valueOf(true));
+        updateLessValueMap(col,value,Boolean.valueOf(true));
+        if(!equalValueMap.containsKey(col)){
+            equalValueMap.put(col, value);
+        }else{
+            if (value.equals(equalValueMap.get(col))){
+                this.noResult=true;
+                return;
+            }
+        }
+        this.fixs.add(col);
+        this.cols.add(col);
+      } else {
+        if((op instanceof GenericUDFOPNotEqual)){
+          if(notEqualMap.get(col)==null){
+            ArrayList<Double> list=new ArrayList<Double>();
+            list.add(value);
+            notEqualMap.put(col,list);
+          }else{
+            notEqualMap.get(col).add(value);
+          }
+          this.cols.add(col); 
+          return; 
+        }
+        if (this.fixs.contains(col)) {
+          return;
+        }
+        this.cols.add(col);
+        if (rightOrder) {
+          if ((op instanceof GenericUDFOPEqualOrGreaterThan)) {
+            updateGreaterValueMap(col,value,Boolean.valueOf(true));
+          } else if ((op instanceof GenericUDFOPGreaterThan)) {
+            updateGreaterValueMap(col,value,Boolean.valueOf(false));
+          } else if ((op instanceof GenericUDFOPEqualOrLessThan)) {
+            updateLessValueMap(col,value,Boolean.valueOf(true));
+          } else if ((op instanceof GenericUDFOPLessThan)) {
+            updateLessValueMap(col,value,Boolean.valueOf(false));
+          }
+        }
+        else if ((op instanceof GenericUDFOPEqualOrGreaterThan)) {
+            updateLessValueMap(col,value,Boolean.valueOf(true));
+        } else if ((op instanceof GenericUDFOPGreaterThan)) {
+            updateLessValueMap(col,value,Boolean.valueOf(false));
+        } else if ((op instanceof GenericUDFOPEqualOrLessThan)) {
+            updateGreaterValueMap(col,value,Boolean.valueOf(true));
+        } else if ((op instanceof GenericUDFOPLessThan)) {
+            updateGreaterValueMap(col,value,Boolean.valueOf(false));
+        }
+      }
+    }
+
+    public void parseExprNodeDesc(ExprNodeDesc node)
+    {
+      GenericUDF udf = ((ExprNodeGenericFuncDesc)node).getGenericUDF();
+      if ((udf instanceof GenericUDFOPOr)) {
+        return;
+      }
+      if ((udf instanceof GenericUDFOPAnd)) {
+        for (ExprNodeDesc ch : node.getChildren()) {
+          parseExprNodeDesc(ch);
+        }
+      }
+      else if (((udf instanceof GenericUDFBaseCompare)) && 
+        (!(udf instanceof GenericUDFOPNotEqual))) {
+        String col = null; String value = null;
+        boolean order = false;
+        for (int i = 0; i < node.getChildren().size(); i++) {
+          ExprNodeDesc ch2 = (ExprNodeDesc)node.getChildren().get(i);
+
+          if ((ch2 instanceof ExprNodeColumnDesc)) {
+            col = ch2.getExprString();
+            if (i == 0) {
+              order = true;
+            }
+          }
+          if (((ch2 instanceof ExprNodeGenericFuncDesc)) && 
+            ((((ExprNodeGenericFuncDesc)ch2).getGenericUDF() instanceof GenericUDFBridge))) {
+            value = new StringBuilder().append("-").append(((ExprNodeDesc)ch2.getChildren().get(0)).getExprString()).toString();
+          }
+
+          if ((ch2 instanceof ExprNodeConstantDesc)) {
+            value = ch2.getExprString();
+          }
+        }
+        if ((col != null) && (value != null))
+          try {
+            updateConditions(col, new Double(value), (GenericUDFBaseCompare)udf, order);
+          } catch (NumberFormatException e) {
+            System.err.println(new StringBuilder().append("NumberForamtException in updateRange:").append(node.getExprString()).toString());
+          }
+      }
+    }
+
+    public NcFileRecordReader(Configuration job, InputSplit genericSplit)
+      throws IOException
+    {
+      FileSplit split = (FileSplit)genericSplit;
+      this.start = split.getStart();
+      this.end = (this.start + split.getLength());
+      this.ncfile = NetcdfFile.open(split.getPath().toString());
+      this.filename = split.getPath().getName();
+      this.seprator = "\t";
+      ArrayList notSkipIDs = ColumnProjectionUtils.getReadColumnIDs(job);
+      String jobID=Utilities.getHiveJobID(job);
+      LinkedHashMap<String, ArrayList<ExprNodeDesc>> pathToNodeDesc = Utilities.getPathToNodeDesc(jobID);
+      String pathString;
+      if (pathToNodeDesc != null) {
+        pathString = split.getPath().toString();
+        for (String key : pathToNodeDesc.keySet())
+        {
+          if (pathString.startsWith(key)) {
+            for (ExprNodeDesc node : (ArrayList<ExprNodeDesc>)pathToNodeDesc.get(key))
+            {
+              parseExprNodeDesc(node);
+            }
+            break;
+          }
+        }
+      }
+      ArrayList varList = (ArrayList)this.ncfile.getVariables();
+
+      if (notSkipIDs.size() == 0) {
+        for (int i = 0; i < varList.size(); i++) {
+          notSkipIDs.add(Integer.valueOf(i));
+        }
+      }
+      Variable mainVar = null;
+      int mainVarPos = 0;
+      this.varArr = new ArrayList();
+      this.vars = new ArrayList();
+      this.dimArr = new ArrayList();
+      Variable var;
+      for (int i = 0; i < notSkipIDs.size(); i++) {
+        var = (Variable)varList.get(((Integer)notSkipIDs.get(i)).intValue());
+        if ((var.getDimensions().size() != 1) || (!var.getDimension(0).getName().equals(var.getName())))
+        {
+          if (mainVar == null) {
+            mainVar = var;
+            mainVarPos = i;
+          } else {
+            List<Dimension> varDs = var.getDimensions();
+            for (Dimension d : varDs) {
+              if (d.getName().equals(mainVar.getName())) {
+                mainVar = var;
+                mainVarPos = i;
+                break;
+              }
+            }
+          }
+        }
+      }
+      if (mainVar == null) {
+        mainVar = (Variable)varList.get(((Integer)notSkipIDs.get(0)).intValue());
+        mainVarPos = 0;
+      }
+      this.vars.add(mainVar);
+
+      this.types = new int[notSkipIDs.size()];
+      this.dimMap = new int[mainVar.getDimensions().size()];
+      this.types[mainVarPos] = 2;
+
+      for (int i = 0; i < notSkipIDs.size(); i++)
+        if (i != mainVarPos)
+        {
+          var = (Variable)varList.get(((Integer)notSkipIDs.get(i)).intValue());
+          boolean isDimension = false;
+          int size = mainVar.getDimensions().size();
+          for (int j = 0; j < size; j++) {
+            if (mainVar.getDimension(j).getName().equals(var.getName())) {
+              this.types[i] = 1;
+              this.dimArr.add(var.read());
+
+              this.dimMap[(this.dimArr.size() - 1)] = j;
+              isDimension = true;
+            }
+          }
+          if (!isDimension)
+          {
+            if (var.getDimensions().size() == mainVar.getDimensions().size()) {
+              boolean isSame = true;
+              for (int j = 0; j < var.getDimensions().size(); j++) {
+                if (!var.getDimension(j).getName().equals(mainVar.getDimension(j).getName())) {
+                  isSame = false;
+                  break;
+                }
+              }
+              if (isSame) {
+                this.vars.add(var);
+
+                this.types[i] = 2;
+              }
+            }
+          }
+        }
+      ArrayList validIDs = new ArrayList();
+      this.shrinkedTypes = new int[this.types.length];
+      int pos = 0;
+      for (int i = 0; i < this.types.length; i++) {
+        if (this.types[i] != 0) {
+          this.shrinkedTypes[pos] = this.types[i];
+          validIDs.add(notSkipIDs.get(i));
+          pos++;
+        }
+      }
+      org.apache.hadoop.hive.contrib.serde2.NcFileSerDe.notSkipIDsFromInputFormat = validIDs;
+      this.validIDsCount = validIDs.size();
+
+      for (String key : this.cols) {
+          evalDimensionVariable(this.ncfile.findVariable(key),key);
+      }
+      if(this.noResult)
+          return;
+
+      int size = mainVar.getDimensions().size();
+      this.dimsSize=size;
+      this.shape = new int[size];
+      this.validShape=new int[size];
+      this.shapeDivider=new int[size];
+      this.validShapeDivider=new int[size];
+      this.totalSize=1;
+      this.validIndexArr=new ArrayList();
+      for (int j = 0; j < size; j++) {
+        Dimension d = (Dimension)mainVar.getDimensions().get(size-1-j);
+        if(this.cols.contains(d.getName())){
+            validIndexArr.add(validIndexes.get(d.getName()));
+        }else{
+            ArrayList<Integer> arr=new ArrayList<Integer>();
+            for(int i=0;i<d.getLength();i++){
+                arr.add(i);
+            }
+            validIndexArr.add(arr);
+        }
+        this.shape[j] = d.getLength();
+        if(validIndexes.get(d.getName())==null)
+            this.validShape[j]=d.getLength();
+        else
+            this.validShape[j]=validIndexes.get(d.getName()).size();
+
+        if(j==0){
+            shapeDivider[j]=1;
+            validShapeDivider[j]=1;
+        }else{
+            shapeDivider[j]=shapeDivider[j-1]*shape[j-1];
+            validShapeDivider[j]=validShapeDivider[j-1]*validShape[j-1];
+        }
+        totalSize*=validShape[j];
+      }
+      System.err.println("total index size: "+totalSize);
+        for (int i = 0; i < this.vars.size(); i++)
+          this.varArr.add(((Variable)this.vars.get(i)).read());
+    }
+
+    public synchronized boolean next(LongWritable key, Text value)
+      throws IOException
+    {
+      boolean getOne=true;
+      StringBuilder result = null;
+      do{
+    	  
+          if (this.noResult || (this.count >= this.totalSize)) {
+            key = null;
+            value = null;
+            return false;
+          }
+          getOne=true;
+          int [] indexArr=new int[this.dimsSize];
+          int remain=this.count;
+          int newIndex=0;
+          for(int i=0;i<this.dimsSize;i++){
+              indexArr[i]=remain/validShapeDivider[this.dimsSize-1-i];
+              remain=remain%validShapeDivider[this.dimsSize-1-i];
+              newIndex+=validIndexArr.get(this.dimsSize-1-i).get(indexArr[i])*shapeDivider[this.dimsSize-1-i];
+          }
+          int dimPos = 0;
+          int varPos = 0;
+          int pos = 0;
+          String varName=null;
+          Object val=null;
+          result = new StringBuilder();
+          for (int i = 0; i < this.validIDsCount; i++) {
+            if(this.shrinkedTypes[i]==1) {
+              pos=this.dimMap[dimPos];
+              result.append(((Array)this.dimArr.get(dimPos)).getObject(validIndexArr.get(this.dimsSize-1-pos).get(indexArr[pos])));
+              dimPos++;
+            }else{
+              varName=this.vars.get(varPos).getName();
+              val=((Array)this.varArr.get(varPos)).getObject(newIndex);
+              if(this.cols.contains(varName)){
+                  Double gValue=greaterValueMap.get(varName);
+                  Double lValue=lessValueMap.get(varName);
+                  ArrayList<Double> nValues=notEqualMap.get(varName);
+                  if(!eval_value(new Double(val.toString()),varName,gValue,lValue,nValues)){
+                    getOne=false;
+                    this.count++;
+                    break;
+                  }
+                  
+              }
+              result.append(val.toString());
+              varPos++;
+            }
+            if (getOne && i != this.validIDsCount - 1) {
+              result.append(this.seprator);
+            }
+          }
+      }while(!getOne);
+      key.set(this.count);
+      value.set(result.toString());
+      this.count+=1; 
+      return true;
+    }
+
+    public LongWritable createKey() {
+      return new LongWritable();
+    }
+
+    public Text createValue() {
+      return new Text();
+    }
+
+    public synchronized long getPos() throws IOException {
+      return this.count;
+    }
+
+    public void close() throws IOException {
+      if (this.ncfile != null)
+        this.ncfile.close();
+    }
+
+    public float getProgress() throws IOException
+    {
+      if (this.count >= this.totalSize) {
+        return 1.0F;
+      }
+      return 1.0F* this.count / (float)this.totalSize;
+    }
+  }
+}
diff -urN src_origin/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/NcFileRangeInputFormat.java src/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/NcFileRangeInputFormat.java
--- src_origin/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/NcFileRangeInputFormat.java	1970-01-01 08:00:00.000000000 +0800
+++ src/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/NcFileRangeInputFormat.java	2014-01-06 00:49:17.543054622 +0800
@@ -0,0 +1,714 @@
+package org.apache.hadoop.hive.contrib.fileformat.netcdf;
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.List;
+import org.apache.commons.logging.Log;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.BlockLocation;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.io.InputFormatChecker;
+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
+import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrLessThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPLessThan;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr;
+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputSplit;
+import org.apache.hadoop.mapred.InvalidInputException;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.net.NetworkTopology;
+import ucar.ma2.Array;
+import ucar.ma2.Index;
+import ucar.ma2.InvalidRangeException;
+import ucar.ma2.Section;
+import ucar.nc2.Dimension;
+import ucar.nc2.NetcdfFile;
+import ucar.nc2.Variable;
+
+public class NcFileRangeInputFormat extends FileInputFormat<LongWritable, Text>
+  implements InputFormatChecker
+{
+  private static final double SPLIT_SLOP = 1.1;
+  private  long minSplitSize = 1;
+  private static final PathFilter hiddenFileFilter = new PathFilter() {
+    public boolean accept(Path p) {
+      String name = p.getName();
+      return (!name.startsWith("_")) && (!name.startsWith("."));
+    }
+  };
+
+  protected boolean isSplitable(FileSystem fs, Path filename)
+  {
+    return false;
+  }
+
+  public RecordReader<LongWritable, Text> getRecordReader(InputSplit split, JobConf conf, Reporter reporter)
+    throws IOException
+  {
+    reporter.setStatus(split.toString());
+    return new NcFileRecordReader(conf, split);
+  }
+
+  protected FileStatus[] listStatus(JobConf job, FileStatus file) throws IOException
+  {
+    Path p = file.getPath();
+
+    List result = new ArrayList();
+    List errors = new ArrayList();
+
+    List filters = new ArrayList();
+    filters.add(hiddenFileFilter);
+    PathFilter jobFilter = getInputPathFilter(job);
+    if (jobFilter != null) {
+      filters.add(jobFilter);
+    }
+    PathFilter inputFilter = new MultiPathFilter(filters);
+    FileSystem fs = p.getFileSystem(job);
+    FileStatus[] matches = fs.globStatus(p, inputFilter);
+    if (matches == null)
+      errors.add(new IOException("Input path does not exist: " + p));
+    else if (matches.length == 0) {
+      errors.add(new IOException("Input Pattern " + p + " matches 0 files"));
+    }
+    else {
+      for (FileStatus globStat : matches) {
+        if (globStat.isDir())
+          for (FileStatus stat : fs.listStatus(globStat.getPath(), inputFilter))
+          {
+            result.add(stat);
+          }
+        else {
+          result.add(globStat);
+        }
+      }
+    }
+    if (!errors.isEmpty()) {
+      throw new InvalidInputException(errors);
+    }
+    return (FileStatus[])result.toArray(new FileStatus[result.size()]);
+  }
+
+  public long getLen(JobConf job, FileStatus file)
+    throws IOException
+  {
+    long all = 0L;
+    if (file.isDir()) {
+      FileStatus[] files = listStatus(job, file);
+      for (FileStatus subfile : files)
+        all += getLen(job, subfile);
+    }
+    else {
+      all += file.getLen();
+    }
+    return all;
+  }
+
+  public ArrayList<FileSplit> getSplits(JobConf job, FileStatus dirFile, long totalSize, int numSplits)
+    throws IOException
+  {
+    ArrayList fsArr = new ArrayList(numSplits);
+    FileStatus[] files = listStatus(job, dirFile);
+    long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
+    long minSize = Math.max(job.getLong("mapred.min.split.size", 1), 1);
+
+    NetworkTopology clusterMap = new NetworkTopology();
+    for (FileStatus file : files) {
+      if (!file.isDir()) {
+        Path path = file.getPath();
+        FileSystem fs = path.getFileSystem(job);
+        long length = file.getLen();
+        BlockLocation[] blkLocations = fs.getFileBlockLocations(file, 0L, length);
+
+        if ((length != 0L) && (isSplitable(fs, path))) {
+          long blockSize = file.getBlockSize();
+          long splitSize = computeSplitSize(goalSize, minSize, blockSize);
+
+          long bytesRemaining = length;
+          while (bytesRemaining / splitSize > 1.1) {
+            String[] splitHosts = getSplitHosts(blkLocations, length - bytesRemaining, splitSize, clusterMap);
+
+            fsArr.add(new FileSplit(path, length - bytesRemaining, splitSize, splitHosts));
+
+            bytesRemaining -= splitSize;
+          }
+
+          if (bytesRemaining != 0L) {
+            fsArr.add(new FileSplit(path, length - bytesRemaining, bytesRemaining, blkLocations[(blkLocations.length - 1)].getHosts()));
+          }
+
+        }
+        else if (length != 0L) {
+          String[] splitHosts = getSplitHosts(blkLocations, 0L, length, clusterMap);
+
+          fsArr.add(new FileSplit(path, 0L, length, splitHosts));
+        }
+        else {
+          fsArr.add(new FileSplit(path, 0L, length, new String[0]));
+        }
+      } else {
+        fsArr.addAll(getSplits(job, file, totalSize, numSplits));
+      }
+
+    }
+
+    return fsArr;
+  }
+
+  public InputSplit[] getSplits(JobConf job, int numSplits)
+    throws IOException
+  {
+    FileStatus[] files = listStatus(job);
+
+    long totalSize = 0L;
+    for (FileStatus file : files) {
+      if (file.isDir()) {
+        totalSize += getLen(job, file);
+      }
+      totalSize += file.getLen();
+    }
+
+    long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
+    long minSize = Math.max(job.getLong("mapred.min.split.size", 1), 1);
+
+    ArrayList splits = new ArrayList(numSplits);
+    NetworkTopology clusterMap = new NetworkTopology();
+    for (FileStatus file : files) {
+      if (!file.isDir()) {
+        Path path = file.getPath();
+        FileSystem fs = path.getFileSystem(job);
+        long length = file.getLen();
+        BlockLocation[] blkLocations = fs.getFileBlockLocations(file, 0L, length);
+
+        if ((length != 0L) && (isSplitable(fs, path))) {
+          long blockSize = file.getBlockSize();
+          long splitSize = computeSplitSize(goalSize, minSize, blockSize);
+
+          long bytesRemaining = length;
+          while (bytesRemaining / splitSize > 1.1D) {
+            String[] splitHosts = getSplitHosts(blkLocations, length - bytesRemaining, splitSize, clusterMap);
+
+            splits.add(new FileSplit(path, length - bytesRemaining, splitSize, splitHosts));
+
+            bytesRemaining -= splitSize;
+          }
+
+          if (bytesRemaining != 0L) {
+            splits.add(new FileSplit(path, length - bytesRemaining, bytesRemaining, blkLocations[(blkLocations.length - 1)].getHosts()));
+          }
+
+        }
+        else if (length != 0L) {
+          String[] splitHosts = getSplitHosts(blkLocations, 0L, length, clusterMap);
+
+          splits.add(new FileSplit(path, 0L, length, splitHosts));
+        }
+        else {
+          splits.add(new FileSplit(path, 0L, length, new String[0]));
+        }
+      } else {
+        splits.addAll(getSplits(job, file, totalSize, numSplits));
+      }
+    }
+    LOG.debug("Total # of splits: " + splits.size());
+    return (InputSplit[])splits.toArray(new FileSplit[splits.size()]);
+  }
+
+  public boolean validateInput(FileSystem fs, HiveConf conf, ArrayList<FileStatus> files)
+    throws IOException
+  {
+    if (files.size() <= 0) {
+      return false;
+    }
+    for (int fileId = 0; fileId < files.size(); fileId++) {
+      try {
+        NetcdfFile ncfile = NetcdfFile.open(((FileStatus)files.get(fileId)).getPath().toString());
+        ncfile.close();
+      } catch (IOException e) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  private static class MultiPathFilter
+    implements PathFilter
+  {
+    private final List<PathFilter> filters;
+
+    public MultiPathFilter(List<PathFilter> filters)
+    {
+      this.filters = filters;
+    }
+
+    public boolean accept(Path path) {
+      for (PathFilter filter : this.filters) {
+        if (!filter.accept(path)) {
+          return false;
+        }
+      }
+      return true;
+    }
+  }
+
+  static class NcFileRecordReader
+    implements RecordReader<LongWritable, Text>
+  {
+    private final long start;
+    private final long end;
+    private NetcdfFile ncfile = null;
+    private final String filename;
+    private int count;
+    private Index idx = null;
+    private final String seprator;
+    private final ArrayList<Array> varArr;
+    private final ArrayList<Variable> vars;
+    private final ArrayList<Array> dimArr;
+    int[] dimMap;
+    int[] types;
+    int[] shrinkedTypes;
+    int validIDsCount;
+    int[] origin;
+    int[] shape;
+    VariableSlicer vs = new VariableSlicer();
+    public HashMap<String,Double> equalValueMap= new HashMap();
+    public HashMap<String, Double> greaterValueMap = new HashMap();
+    public HashMap<String, Boolean> greaterBoolMap = new HashMap();
+    public HashMap<String, Boolean> lessBoolMap = new HashMap();
+    public HashMap<String, Double> lessValueMap = new HashMap();
+    public HashMap<String, ArrayList<Double>> notEqualMap = new HashMap();
+    public HashSet<String> fixs = new HashSet();
+    public HashSet<String> cols = new HashSet();
+    boolean noResult=false;
+    boolean notEqual(Double input,Double num){
+        return !input.equals(num)?true:false;
+    }
+    boolean greaterEqual(Double input,Double num){
+        return input>=num?true:false;
+    }
+    boolean greater(Double input,Double num){
+        return input>num?true:false;
+    }
+    boolean lessEqual(Double input,Double num){
+        return input<=num?true:false;
+    }
+    boolean less(Double input,Double num){
+        return input<num?true:false;
+    }
+   
+    boolean eval_value(Double val,String col,Double gValue,Double lValue,ArrayList<Double> nValues){
+        if(gValue!=null){
+            if(greaterBoolMap.get(col)==true){
+                if(!greaterEqual(val,gValue)){
+                    return false;
+                }
+            }else{
+                if(!greater(val,gValue)){
+                    return false;
+                }
+            }
+        }
+        if(lValue!=null){
+            if(lessBoolMap.get(col)==true){
+                if(!lessEqual(val,lValue)){
+                    return false;
+                }
+            }else{
+                if(!less(val,lValue)){
+                    return false;
+                }
+            }
+        }
+        if(nValues!=null){
+            for(Double d:nValues){
+                if(!notEqual(val,d)){
+                    return false;
+                }
+            }
+        }
+        return true;
+    }
+    void updateGreaterValueMap(String col,Double value,Boolean equal){
+        Double old=greaterValueMap.get(col);
+        if(old==null){
+            this.greaterValueMap.put(col, value);
+            this.greaterBoolMap.put(col, equal);
+        }else{
+            if(value>old){
+                this.greaterValueMap.put(col, value);
+                this.greaterBoolMap.put(col, equal);
+            }else if(value.equals(old)){
+                if(!equal)
+                    this.greaterBoolMap.put(col,equal);
+            }
+        }
+    }
+    void updateLessValueMap(String col,Double value,Boolean equal){
+        Double old=lessValueMap.get(col);
+        if(old==null){
+            this.lessValueMap.put(col, value);
+            this.lessBoolMap.put(col, equal);
+        }else{
+            if(value<old){
+                this.lessValueMap.put(col, value);
+                this.lessBoolMap.put(col, equal);
+            }else if(value.equals(old)){
+                if(!equal)
+                    this.lessBoolMap.put(col,equal);
+            }
+        }
+    }
+
+    public void updateRange(String col, Double value, GenericUDFBaseCompare op, boolean rightOrder) {
+      if ((op instanceof GenericUDFOPEqual)) {
+        updateGreaterValueMap(col,value,Boolean.valueOf(true));
+        updateLessValueMap(col,value,Boolean.valueOf(true));
+        if(!equalValueMap.containsKey(col)){
+            equalValueMap.put(col, value);
+        }else{
+            if (value.equals(equalValueMap.get(col))){
+                this.noResult=true;
+                return;
+            }
+        }
+        this.fixs.add(col);
+        this.cols.add(col);
+      } else {
+    	  if((op instanceof GenericUDFOPNotEqual)){
+              if(notEqualMap.get(col)==null){
+                ArrayList<Double> list=new ArrayList<Double>();
+                list.add(value);
+                notEqualMap.put(col,list);
+              }else{
+                notEqualMap.get(col).add(value);
+              }
+              this.cols.add(col); 
+              return; 
+            }
+        if (this.fixs.contains(col)) {
+          return;
+        }
+        this.cols.add(col);
+        if (rightOrder) {
+          if ((op instanceof GenericUDFOPEqualOrGreaterThan)) {
+            updateGreaterValueMap(col,value,Boolean.valueOf(true));
+          } else if ((op instanceof GenericUDFOPGreaterThan)) {
+            updateGreaterValueMap(col,value,Boolean.valueOf(false));
+          } else if ((op instanceof GenericUDFOPEqualOrLessThan)) {
+            updateLessValueMap(col,value,Boolean.valueOf(true));
+          } else if ((op instanceof GenericUDFOPLessThan)) {
+            updateLessValueMap(col,value,Boolean.valueOf(false));
+          }
+        }
+        else if ((op instanceof GenericUDFOPEqualOrGreaterThan)) {
+            updateLessValueMap(col,value,Boolean.valueOf(true));
+        } else if ((op instanceof GenericUDFOPGreaterThan)) {
+            updateLessValueMap(col,value,Boolean.valueOf(false));
+        } else if ((op instanceof GenericUDFOPEqualOrLessThan)) {
+            updateGreaterValueMap(col,value,Boolean.valueOf(true));
+        } else if ((op instanceof GenericUDFOPLessThan)) {
+            updateGreaterValueMap(col,value,Boolean.valueOf(false));
+        }
+      }
+    }
+
+    public void parseExprNodeDesc(ExprNodeDesc node)
+    {
+      GenericUDF udf = ((ExprNodeGenericFuncDesc)node).getGenericUDF();
+      if ((udf instanceof GenericUDFOPOr)) {
+        return;
+      }
+      if ((udf instanceof GenericUDFOPAnd)) {
+        for (ExprNodeDesc ch : node.getChildren()) {
+          parseExprNodeDesc(ch);
+        }
+      }
+      else if (((udf instanceof GenericUDFBaseCompare)) && 
+        (!(udf instanceof GenericUDFOPNotEqual))) {
+        String col = null; String value = null;
+        boolean order = false;
+        for (int i = 0; i < node.getChildren().size(); i++) {
+          ExprNodeDesc ch2 = (ExprNodeDesc)node.getChildren().get(i);
+
+          if ((ch2 instanceof ExprNodeColumnDesc)) {
+            col = ch2.getExprString();
+            if (i == 0) {
+              order = true;
+            }
+          }
+          if (((ch2 instanceof ExprNodeGenericFuncDesc)) && 
+            ((((ExprNodeGenericFuncDesc)ch2).getGenericUDF() instanceof GenericUDFBridge))) {
+            value = new StringBuilder().append("-").append(((ExprNodeDesc)ch2.getChildren().get(0)).getExprString()).toString();
+          }
+
+          if ((ch2 instanceof ExprNodeConstantDesc)) {
+            value = ch2.getExprString();
+          }
+        }
+        if ((col != null) && (value != null))
+          try {
+            updateRange(col, new Double(value), (GenericUDFBaseCompare)udf, order);
+          } catch (NumberFormatException e) {
+            System.err.println(new StringBuilder().append("NumberForamtException in updateRange:").append(node.getExprString()).toString());
+          }
+      }
+    }
+
+    public NcFileRecordReader(Configuration job, InputSplit genericSplit)
+      throws IOException
+    {
+      FileSplit split = (FileSplit)genericSplit;
+      this.start = split.getStart();
+      this.end = (this.start + split.getLength());
+      this.ncfile = NetcdfFile.open(split.getPath().toString());
+      this.filename = split.getPath().getName();
+      this.seprator = "\t";
+      ArrayList notSkipIDs = ColumnProjectionUtils.getReadColumnIDs(job);
+      String jobID=Utilities.getHiveJobID(job);
+      LinkedHashMap<String, ArrayList<ExprNodeDesc>> pathToNodeDesc = Utilities.getPathToNodeDesc(jobID);
+      String pathString;
+      if (pathToNodeDesc != null) {
+        pathString = split.getPath().toString();
+        for (String key : pathToNodeDesc.keySet())
+        {
+          if (pathString.startsWith(key)) {
+            for (ExprNodeDesc node : (ArrayList<ExprNodeDesc>)pathToNodeDesc.get(key))
+            {
+              parseExprNodeDesc(node);
+            }
+            break;
+          }
+        }
+      }
+      ArrayList varList = (ArrayList)this.ncfile.getVariables();
+
+      if (notSkipIDs.size() == 0) {
+        for (int i = 0; i < varList.size(); i++) {
+          notSkipIDs.add(Integer.valueOf(i));
+        }
+      }
+      Variable mainVar = null;
+      int mainVarPos = 0;
+      this.varArr = new ArrayList();
+      this.vars = new ArrayList();
+      this.dimArr = new ArrayList();
+      Variable var;
+      for (int i = 0; i < notSkipIDs.size(); i++) {
+        var = (Variable)varList.get(((Integer)notSkipIDs.get(i)).intValue());
+        if ((var.getDimensions().size() != 1) || (!var.getDimension(0).getName().equals(var.getName())))
+        {
+          if (mainVar == null) {
+            mainVar = var;
+            mainVarPos = i;
+          } else {
+            List<Dimension> varDs = var.getDimensions();
+            for (Dimension d : varDs) {
+              if (d.getName().equals(mainVar.getName())) {
+                mainVar = var;
+                mainVarPos = i;
+                break;
+              }
+            }
+          }
+        }
+      }
+      if (mainVar == null) {
+        mainVar = (Variable)varList.get(((Integer)notSkipIDs.get(0)).intValue());
+        mainVarPos = 0;
+      }
+      this.vars.add(mainVar);
+
+      this.types = new int[notSkipIDs.size()];
+      this.dimMap = new int[mainVar.getDimensions().size()];
+      this.types[mainVarPos] = 2;
+
+      for (int i = 0; i < notSkipIDs.size(); i++)
+        if (i != mainVarPos)
+        {
+          var = (Variable)varList.get(((Integer)notSkipIDs.get(i)).intValue());
+          boolean isDimension = false;
+          int size = mainVar.getDimensions().size();
+          for (int j = 0; j < size; j++) {
+            if (mainVar.getDimension(j).getName().equals(var.getName())) {
+              this.types[i] = 1;
+              this.dimArr.add(var.read());
+
+              this.dimMap[(this.dimArr.size() - 1)] = j;
+              isDimension = true;
+            }
+          }
+          if (!isDimension)
+          {
+            if (var.getDimensions().size() == mainVar.getDimensions().size()) {
+              boolean isSame = true;
+              for (int j = 0; j < var.getDimensions().size(); j++) {
+                if (!var.getDimension(j).getName().equals(mainVar.getDimension(j).getName())) {
+                  isSame = false;
+                  break;
+                }
+              }
+              if (isSame) {
+                this.vars.add(var);
+
+                this.types[i] = 2;
+              }
+            }
+          }
+        }
+      ArrayList validIDs = new ArrayList();
+      this.shrinkedTypes = new int[this.types.length];
+      int pos = 0;
+      for (int i = 0; i < this.types.length; i++) {
+        if (this.types[i] != 0) {
+          this.shrinkedTypes[pos] = this.types[i];
+          validIDs.add(notSkipIDs.get(i));
+          pos++;
+        }
+      }
+      org.apache.hadoop.hive.contrib.serde2.NcFileSerDe.notSkipIDsFromInputFormat = validIDs;
+      this.validIDsCount = validIDs.size();
+
+      for (String key : this.cols) {
+        this.vs.process(this.ncfile.findVariable(key), (Double)this.greaterValueMap.get(key), (Boolean)this.greaterBoolMap.get(key), (Double)this.lessValueMap.get(key), (Boolean)this.lessBoolMap.get(key));
+
+        if (!this.vs.getHasResult()) {
+          System.err.println(new StringBuilder().append(split.getPath().toString()).append(":no result returned!").toString());
+          return;
+        }
+      }
+      HashMap beginMap = this.vs.getBeginMap();
+      HashMap endMap = this.vs.getEndMap();
+      if(this.noResult)
+          return;
+      
+      int size = mainVar.getDimensions().size();
+      this.origin = new int[size];
+      this.shape = new int[size];
+      for (int j = 0; j < size; j++) {
+        Dimension d = (Dimension)mainVar.getDimensions().get(j);
+        if (!beginMap.containsKey(d.getName())) {
+          beginMap.put(d.getName(), Integer.valueOf(0));
+          endMap.put(d.getName(), Integer.valueOf(d.getLength() - 1));
+          this.origin[j] = 0;
+          this.shape[j] = d.getLength();
+        } else {
+          this.origin[j] = ((Integer)beginMap.get(d.getName())).intValue();
+          this.shape[j] = (((Integer)endMap.get(d.getName())).intValue() - ((Integer)beginMap.get(d.getName())).intValue() + 1);
+        }
+        System.err.println(new StringBuilder().append(d.getName()).append(": idx->").append(j).append(" origin->").append(this.origin[j]).append(" shape->").append(this.shape[j]).toString());
+      }
+      try {
+        Section sc = new Section(this.origin, this.shape);
+        for (int i = 0; i < this.vars.size(); i++)
+          this.varArr.add(((Variable)this.vars.get(i)).read(sc));
+      }
+      catch (InvalidRangeException e)
+      {
+        e.printStackTrace();
+      }
+      this.idx = ((Array)this.varArr.get(0)).getIndex();
+    }
+
+    public synchronized boolean next(LongWritable key, Text value)
+      throws IOException{
+      boolean getOne=true;
+      StringBuilder result = null;
+      do{
+          if (this.noResult||(!this.vs.getHasResult()) || (this.count >= this.idx.getSize())) {
+            key = null;
+            value = null;
+            return false;
+          }
+          getOne=true;
+          int[] idxCount = this.idx.getCurrentCounter();
+          int dimPos = 0;
+          int varPos = 0;
+          String varName=null;
+          Object val=null;
+          result = new StringBuilder();
+          for (int i = 0; i < this.validIDsCount; i++) {
+            if(this.shrinkedTypes[i]==1) {
+              result.append(((Array)this.dimArr.get(dimPos)).getObject(this.origin[this.dimMap[dimPos]] + idxCount[this.dimMap[dimPos]]));
+              dimPos++;
+            }else{
+              varName=this.vars.get(varPos).getName();
+              val=((Array)this.varArr.get(varPos)).getObject(this.idx);
+              if(this.cols.contains(varName)){
+                  Double gValue=greaterValueMap.get(varName);
+                  Double lValue=lessValueMap.get(varName);
+                  ArrayList<Double> nValues=notEqualMap.get(varName);
+                  if(!eval_value(new Double(val.toString()),varName,gValue,lValue,nValues)){
+                    getOne=false;
+                    this.idx.incr();
+                    this.count += 1;
+//                    System.err.println("filtered value :"+val);
+                    break;
+                  }
+              }
+              result.append(val.toString());
+              varPos++;
+            }
+
+            if (getOne && i != this.validIDsCount - 1) {
+              result.append(this.seprator);
+            }
+          }
+      }while(!getOne);
+      key.set(this.count);
+      value.set(result.toString());
+      this.idx.incr();
+      this.count += 1;
+      return true;
+    }
+
+    public LongWritable createKey() {
+      return new LongWritable();
+    }
+
+    public Text createValue() {
+      return new Text();
+    }
+
+    public synchronized long getPos() throws IOException {
+      return this.count;
+    }
+
+    public void close() throws IOException {
+      if (this.ncfile != null)
+        this.ncfile.close();
+    }
+
+    public float getProgress() throws IOException
+    {
+      if (this.idx == null) {
+        return 1.0F;
+      }
+      return 1.0F * this.count / (float)this.idx.getSize();
+    }
+  }
+}
diff -urN src_origin/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/NcFileSimpleInputFormat.java src/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/NcFileSimpleInputFormat.java
--- src_origin/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/NcFileSimpleInputFormat.java	1970-01-01 08:00:00.000000000 +0800
+++ src/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/NcFileSimpleInputFormat.java	2013-04-10 22:22:32.248871451 +0800
@@ -0,0 +1,435 @@
+package org.apache.hadoop.hive.contrib.fileformat.netcdf;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import org.apache.commons.logging.Log;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.BlockLocation;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.io.InputFormatChecker;
+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputSplit;
+import org.apache.hadoop.mapred.InvalidInputException;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.net.NetworkTopology;
+import ucar.ma2.Array;
+import ucar.ma2.Index;
+import ucar.nc2.Dimension;
+import ucar.nc2.NetcdfFile;
+import ucar.nc2.Variable;
+
+public class NcFileSimpleInputFormat extends FileInputFormat<LongWritable, Text>
+  implements InputFormatChecker
+{
+  private static final double SPLIT_SLOP = 1.1;
+  private long minSplitSize = 1;
+  private static final PathFilter hiddenFileFilter = new PathFilter() {
+    public boolean accept(Path p) {
+      String name = p.getName();
+      return (!name.startsWith("_")) && (!name.startsWith("."));
+    }
+  };
+
+  protected boolean isSplitable(FileSystem fs, Path filename)
+  {
+    return false;
+  }
+
+  public RecordReader<LongWritable, Text> getRecordReader(InputSplit split, JobConf conf, Reporter reporter)
+    throws IOException
+  {
+    reporter.setStatus(split.toString());
+    return new NcFileRecordReader(conf, split);
+  }
+
+  protected FileStatus[] listStatus(JobConf job, FileStatus file) throws IOException
+  {
+    Path p = file.getPath();
+
+    List result = new ArrayList();
+    List errors = new ArrayList();
+
+    List filters = new ArrayList();
+    filters.add(hiddenFileFilter);
+    PathFilter jobFilter = getInputPathFilter(job);
+    if (jobFilter != null) {
+      filters.add(jobFilter);
+    }
+    PathFilter inputFilter = new MultiPathFilter(filters);
+    FileSystem fs = p.getFileSystem(job);
+    FileStatus[] matches = fs.globStatus(p, inputFilter);
+    if (matches == null)
+      errors.add(new IOException("Input path does not exist: " + p));
+    else if (matches.length == 0) {
+      errors.add(new IOException("Input Pattern " + p + " matches 0 files"));
+    }
+    else {
+      for (FileStatus globStat : matches) {
+        if (globStat.isDir())
+          for (FileStatus stat : fs.listStatus(globStat.getPath(), inputFilter))
+          {
+            result.add(stat);
+          }
+        else {
+          result.add(globStat);
+        }
+      }
+    }
+    if (!errors.isEmpty()) {
+      throw new InvalidInputException(errors);
+    }
+    return (FileStatus[])result.toArray(new FileStatus[result.size()]);
+  }
+
+  public long getLen(JobConf job, FileStatus file)
+    throws IOException
+  {
+    long all = 0L;
+    if (file.isDir()) {
+      FileStatus[] files = listStatus(job, file);
+      for (FileStatus subfile : files)
+        all += getLen(job, subfile);
+    }
+    else {
+      all += file.getLen();
+    }
+    return all;
+  }
+
+  public ArrayList<FileSplit> getSplits(JobConf job, FileStatus dirFile, long totalSize, int numSplits)
+    throws IOException
+  {
+    ArrayList fsArr = new ArrayList(numSplits);
+    FileStatus[] files = listStatus(job, dirFile);
+    long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
+    long minSize = Math.max(job.getLong("mapred.min.split.size", 1), 1);
+
+    NetworkTopology clusterMap = new NetworkTopology();
+    for (FileStatus file : files) {
+      if (!file.isDir()) {
+        Path path = file.getPath();
+        FileSystem fs = path.getFileSystem(job);
+        long length = file.getLen();
+        BlockLocation[] blkLocations = fs.getFileBlockLocations(file, 0L, length);
+
+        if ((length != 0L) && (isSplitable(fs, path))) {
+          long blockSize = file.getBlockSize();
+          long splitSize = computeSplitSize(goalSize, minSize, blockSize);
+
+          long bytesRemaining = length;
+          while (bytesRemaining / splitSize > 1.1D) {
+            String[] splitHosts = getSplitHosts(blkLocations, length - bytesRemaining, splitSize, clusterMap);
+
+            fsArr.add(new FileSplit(path, length - bytesRemaining, splitSize, splitHosts));
+
+            bytesRemaining -= splitSize;
+          }
+
+          if (bytesRemaining != 0L) {
+            fsArr.add(new FileSplit(path, length - bytesRemaining, bytesRemaining, blkLocations[(blkLocations.length - 1)].getHosts()));
+          }
+
+        }
+        else if (length != 0L) {
+          String[] splitHosts = getSplitHosts(blkLocations, 0L, length, clusterMap);
+
+          fsArr.add(new FileSplit(path, 0L, length, splitHosts));
+        }
+        else {
+          fsArr.add(new FileSplit(path, 0L, length, new String[0]));
+        }
+      } else {
+        fsArr.addAll(getSplits(job, file, totalSize, numSplits));
+      }
+
+    }
+
+    return fsArr;
+  }
+
+  public InputSplit[] getSplits(JobConf job, int numSplits)
+    throws IOException
+  {
+    FileStatus[] files = listStatus(job);
+
+    long totalSize = 0L;
+    for (FileStatus file : files) {
+      if (file.isDir()) {
+        totalSize += getLen(job, file);
+      }
+      totalSize += file.getLen();
+    }
+
+    long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
+    long minSize = Math.max(job.getLong("mapred.min.split.size", 1), 1);
+
+    ArrayList splits = new ArrayList(numSplits);
+    NetworkTopology clusterMap = new NetworkTopology();
+    for (FileStatus file : files) {
+      if (!file.isDir()) {
+        Path path = file.getPath();
+        FileSystem fs = path.getFileSystem(job);
+        long length = file.getLen();
+        BlockLocation[] blkLocations = fs.getFileBlockLocations(file, 0L, length);
+
+        if ((length != 0L) && (isSplitable(fs, path))) {
+          long blockSize = file.getBlockSize();
+          long splitSize = computeSplitSize(goalSize, minSize, blockSize);
+
+          long bytesRemaining = length;
+          while (bytesRemaining / splitSize > 1.1D) {
+            String[] splitHosts = getSplitHosts(blkLocations, length - bytesRemaining, splitSize, clusterMap);
+
+            splits.add(new FileSplit(path, length - bytesRemaining, splitSize, splitHosts));
+
+            bytesRemaining -= splitSize;
+          }
+
+          if (bytesRemaining != 0L) {
+            splits.add(new FileSplit(path, length - bytesRemaining, bytesRemaining, blkLocations[(blkLocations.length - 1)].getHosts()));
+          }
+
+        }
+        else if (length != 0L) {
+          String[] splitHosts = getSplitHosts(blkLocations, 0L, length, clusterMap);
+
+          splits.add(new FileSplit(path, 0L, length, splitHosts));
+        }
+        else {
+          splits.add(new FileSplit(path, 0L, length, new String[0]));
+        }
+      } else {
+        splits.addAll(getSplits(job, file, totalSize, numSplits));
+      }
+    }
+    LOG.debug("Total # of splits: " + splits.size());
+    return (InputSplit[])splits.toArray(new FileSplit[splits.size()]);
+  }
+
+  public boolean validateInput(FileSystem fs, HiveConf conf, ArrayList<FileStatus> files)
+    throws IOException
+  {
+    if (files.size() <= 0) {
+      return false;
+    }
+    for (int fileId = 0; fileId < files.size(); fileId++) {
+      try {
+        NetcdfFile ncfile = NetcdfFile.open(((FileStatus)files.get(fileId)).getPath().toString());
+        ncfile.close();
+      } catch (IOException e) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  private static class MultiPathFilter
+    implements PathFilter
+  {
+    private final List<PathFilter> filters;
+
+    public MultiPathFilter(List<PathFilter> filters)
+    {
+      this.filters = filters;
+    }
+
+    public boolean accept(Path path) {
+      for (PathFilter filter : this.filters) {
+        if (!filter.accept(path)) {
+          return false;
+        }
+      }
+      return true;
+    }
+  }
+
+  static class NcFileRecordReader
+    implements RecordReader<LongWritable, Text>
+  {
+    private final long start;
+    private final long end;
+    private NetcdfFile ncfile = null;
+    private boolean isReaded;
+    private final String filename;
+    private int count;
+    private final Index idx;
+    private final String seprator;
+    private final ArrayList<Array> varArr;
+    private final ArrayList<Array> dimArr;
+    int[] dimMap;
+    int[] types;
+    int[] shrinkedTypes;
+    int validIDsCount;
+
+    public NcFileRecordReader(Configuration job, InputSplit genericSplit)
+      throws IOException
+    {
+      FileSplit split = (FileSplit)genericSplit;
+      this.start = split.getStart();
+      this.end = (this.start + split.getLength());
+      this.ncfile = NetcdfFile.open(split.getPath().toString());
+      this.filename = split.getPath().getName();
+      this.isReaded = false;
+      this.seprator = "\t";
+      ArrayList notSkipIDs = ColumnProjectionUtils.getReadColumnIDs(job);
+
+      ArrayList varList = (ArrayList)this.ncfile.getVariables();
+
+      if (notSkipIDs.size() == 0) {
+        for (int i = 0; i < varList.size(); i++) {
+          notSkipIDs.add(Integer.valueOf(i));
+        }
+      }
+      Variable mainVar = null;
+      int mainVarPos = 0;
+      this.varArr = new ArrayList();
+      this.dimArr = new ArrayList();
+      Variable var;
+      for (int i = 0; i < notSkipIDs.size(); i++) {
+        var = (Variable)varList.get(((Integer)notSkipIDs.get(i)).intValue());
+        if ((var.getDimensions().size() != 1) || (!var.getDimension(0).getName().equals(var.getName())))
+        {
+          if (mainVar == null) {
+            mainVar = var;
+            mainVarPos = i;
+          } else {
+            List<Dimension> varDs = var.getDimensions();
+            for (Dimension d : varDs) {
+              if (d.getName().equals(mainVar.getName())) {
+                mainVar = var;
+                mainVarPos = i;
+                break;
+              }
+            }
+          }
+        }
+      }
+      if (mainVar == null) {
+        mainVar = (Variable)varList.get(((Integer)notSkipIDs.get(0)).intValue());
+        mainVarPos = 0;
+      }
+      this.varArr.add(mainVar.read());
+      this.types = new int[notSkipIDs.size()];
+      this.dimMap = new int[mainVar.getDimensions().size()];
+      this.types[mainVarPos] = 2;
+
+      for (int i = 0; i < notSkipIDs.size(); i++) {
+        if (i != mainVarPos)
+        {
+          var = (Variable)varList.get(((Integer)notSkipIDs.get(i)).intValue());
+          boolean isDimension = false;
+          int size = mainVar.getDimensions().size();
+          for (int j = 0; j < size; j++) {
+            if (mainVar.getDimension(j).getName().equals(var.getName())) {
+              this.types[i] = 1;
+              this.dimArr.add(var.read());
+
+              this.dimMap[(this.dimArr.size() - 1)] = j;
+              isDimension = true;
+            }
+          }
+          if ((!isDimension) && 
+            (var.getDimensions().size() == mainVar.getDimensions().size())) {
+            boolean isSame = true;
+            for (int j = 0; j < var.getDimensions().size(); j++) {
+              if (!var.getDimension(j).getName().equals(mainVar.getDimension(j).getName())) {
+                isSame = false;
+                break;
+              }
+            }
+            if (isSame) {
+              this.varArr.add(var.read());
+              this.types[i] = 2;
+            }
+          }
+        }
+      }
+      ArrayList validIDs = new ArrayList();
+      this.shrinkedTypes = new int[this.types.length];
+      int pos = 0;
+      for (int i = 0; i < this.types.length; i++) {
+        if (this.types[i] != 0) {
+          this.shrinkedTypes[pos] = this.types[i];
+          validIDs.add(notSkipIDs.get(i));
+          pos++;
+        }
+      }
+      org.apache.hadoop.hive.contrib.serde2.NcFileSerDe.notSkipIDsFromInputFormat = validIDs;
+      this.validIDsCount = validIDs.size();
+      this.idx = mainVar.read().getIndex();
+    }
+
+    public synchronized boolean next(LongWritable key, Text value)
+      throws IOException
+    {
+      if (this.count >= this.idx.getSize()) {
+        this.isReaded = true;
+        key = null;
+        value = null;
+
+        return false;
+      }
+
+      int[] idxCount = this.idx.getCurrentCounter();
+
+      StringBuilder result = new StringBuilder();
+      int dimPos = 0;
+      int varPos = 0;
+      for (int i = 0; i < this.validIDsCount; i++) {
+        switch (this.shrinkedTypes[i]) {
+        case 1:
+          result.append(((Array)this.dimArr.get(dimPos)).getObject(idxCount[this.dimMap[dimPos]]));
+          dimPos++;
+          break;
+        case 2:
+          result.append(((Array)this.varArr.get(varPos)).getObject(this.idx));
+          varPos++;
+        }
+
+        if (i != this.validIDsCount - 1) {
+          result.append(this.seprator);
+        }
+      }
+
+      key.set(this.count);
+      value.set(result.toString());
+
+      this.idx.incr();
+      this.count += 1;
+      return true;
+    }
+
+    public LongWritable createKey() {
+      return new LongWritable();
+    }
+
+    public Text createValue() {
+      return new Text();
+    }
+
+    public synchronized long getPos() throws IOException {
+      return this.count;
+    }
+
+    public void close() throws IOException {
+      if (this.ncfile != null)
+        this.ncfile.close();
+    }
+
+    public float getProgress() throws IOException
+    {
+      return 1.0F * this.count / (float)this.idx.getSize();
+    }
+  }
+}
diff -urN src_origin/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/VariableSlicer.java src/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/VariableSlicer.java
--- src_origin/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/VariableSlicer.java	1970-01-01 08:00:00.000000000 +0800
+++ src/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/netcdf/VariableSlicer.java	2013-04-10 22:22:32.252871451 +0800
@@ -0,0 +1,241 @@
+package org.apache.hadoop.hive.contrib.fileformat.netcdf;
+
+import java.io.IOException;
+import java.util.HashMap;
+import ucar.ma2.Array;
+import ucar.ma2.Index;
+import ucar.nc2.Variable;
+
+public class VariableSlicer
+{
+  private boolean hasResult;
+  private final HashMap<String, Integer> beginMap;
+  private final HashMap<String, Integer> endMap;
+
+  public VariableSlicer()
+  {
+    this.hasResult = true;
+    this.beginMap = new HashMap<String,Integer>();
+    this.endMap = new HashMap<String,Integer>();
+  }
+  public boolean getHasResult() {
+    return this.hasResult;
+  }
+  public int binarySearchAsc(Array arr, double value) {
+    int begin = 0;
+    int end = (int)(arr.getIndex().getSize() - 1);
+    Index idx = arr.getIndex();
+
+    while (end >= begin) {
+      int pos = begin + (end - begin >> 1);
+      idx.set0(pos);
+      double val = new Double(arr.getObject(idx).toString()).doubleValue();
+      if (val < value)
+        begin = pos + 1;
+      else if (val > value)
+        end = pos - 1;
+      else {
+        return pos;
+      }
+    }
+    return begin;
+  }
+  public int binarySearchDesc(Array arr, double value) {
+    int begin = 0;
+    int end = (int)(arr.getIndex().getSize() - 1);
+    Index idx = arr.getIndex();
+
+    while (end >= begin) {
+      int pos = (end + begin) / 2;
+      idx.set0(pos);
+      double val = new Double(arr.getObject(idx).toString()).doubleValue();
+      if (val == value)
+        return pos;
+      if (val > value)
+        begin = pos + 1;
+      else {
+        end = pos - 1;
+      }
+    }
+    return begin;
+  }
+  public int lessPos(Array arr, double value, int binaryPos) {
+    Index idx = arr.getIndex();
+    int pos = binaryPos;
+    int bound = (int)(idx.getSize() - 1);
+    if ((binaryPos < 0) || (binaryPos > bound)) {
+      return binaryPos;
+    }
+    idx.set0(binaryPos);
+    double val = new Double(arr.getObject(idx).toString()).doubleValue();
+    if (val != value)
+      return binaryPos - 1;
+    do
+    {
+      pos--;
+      if (pos < 0) {
+        return -1;
+      }
+      idx.set0(pos);
+      val = new Double(arr.getObject(idx).toString()).doubleValue();
+    }
+    while ((val == value) && (pos >= 0));
+    return pos;
+  }
+
+  public int greaterPos(Array arr, double value, int binaryPos)
+  {
+    Index idx = arr.getIndex();
+    int pos = binaryPos;
+    int bound = (int)(idx.getSize() - 1);
+    if ((binaryPos < 0) || (binaryPos > bound)) {
+      return binaryPos;
+    }
+    idx.set0(binaryPos);
+    double val = new Double(arr.getObject(idx).toString()).doubleValue();
+    if (val != value)
+      return binaryPos;
+    do
+    {
+      pos++;
+      if (pos > bound) {
+        return bound + 1;
+      }
+      idx.set0(pos);
+      val = new Double(arr.getObject(idx).toString()).doubleValue();
+    }while ((val == value) && (pos <= bound));
+    return pos;
+  }
+
+  public int lessEqualPos(Array arr, double value, int binaryPos) {
+    Index idx = arr.getIndex();
+    int pos = binaryPos;
+    int bound = (int)(idx.getSize() - 1);
+    if ((binaryPos < 0) || (binaryPos > bound)) {
+      return binaryPos;
+    }
+    idx.set0(binaryPos);
+    double val = new Double(arr.getObject(idx).toString()).doubleValue();
+
+    if (val != value)
+      return binaryPos - 1;
+    do
+    {
+      pos++;
+      if (pos > bound) {
+        return bound;
+      }
+      idx.set0(pos);
+      val = new Double(arr.getObject(idx).toString()).doubleValue();
+    }
+    while ((val == value) && (pos <= bound));
+    return pos - 1;
+  }
+
+  public int greaterEqualPos(Array arr, double value, int binaryPos)
+  {
+    Index idx = arr.getIndex();
+    int pos = binaryPos;
+    int bound = (int)(idx.getSize() - 1);
+    if ((binaryPos < 0) || (binaryPos > bound)) {
+      return binaryPos;
+    }
+    idx.set0(binaryPos);
+    double val = new Double(arr.getObject(idx).toString()).doubleValue();
+    if (val != value)
+      return binaryPos;
+    do
+    {
+      pos--;
+      if (pos < 0) {
+        return 0;
+      }
+      idx.set0(pos);
+      val = new Double(arr.getObject(idx).toString()).doubleValue();
+    }while ((val == value) && (pos >= 0));
+    return pos + 1;
+  }
+
+  public void process(Variable v, Double greaterValue, Boolean greaterEqual, Double lessValue, Boolean lessEqual)
+  {
+    if(v.getDimensions().size()>1)
+        return;
+    double minValue =Double.MIN_VALUE;
+    boolean minEqual = true;
+    double maxValue = Double.MAX_VALUE;
+    boolean maxEqual = true;
+    if (greaterValue != null) {
+      minValue = greaterValue.doubleValue();
+      minEqual = greaterEqual.booleanValue();
+    }
+    if (lessValue != null) {
+      maxValue = lessValue.doubleValue();
+      maxEqual = lessEqual.booleanValue();
+    }
+    setMaps(v, minValue, minEqual, maxValue, maxEqual);
+  }
+  public void setMaps(Variable v, double minValue, boolean minEqual, double maxValue, boolean maxEqual) {
+    try {
+      Array arr = v.read();
+      Index idx = arr.getIndex();
+      int beginIndex = 0;
+      int endIndex = (int)(idx.getSize() - 1);
+      int bound = endIndex;
+      idx.set0(beginIndex);
+      double first = new Double(arr.getObject(idx).toString()).doubleValue();
+      idx.set0(endIndex);
+      double last = new Double(arr.getObject(idx).toString()).doubleValue();
+      if (first <= last) {
+        int binaryPos1 = binarySearchAsc(arr, minValue);
+        int binaryPos2 = binarySearchAsc(arr, maxValue);
+
+        if (minEqual)
+          beginIndex = greaterEqualPos(arr, minValue, binaryPos1);
+        else {
+          beginIndex = greaterPos(arr, minValue, binaryPos1);
+        }
+        if (maxEqual)
+          endIndex = lessEqualPos(arr, maxValue, binaryPos2);
+        else
+          endIndex = lessPos(arr, maxValue, binaryPos2);
+      }
+      else
+      {
+        int binaryPos1 = binarySearchDesc(arr, maxValue);
+        int binaryPos2 = binarySearchDesc(arr, minValue);
+
+        if (maxEqual)
+          beginIndex = greaterEqualPos(arr, maxValue, binaryPos1);
+        else {
+          beginIndex = greaterPos(arr, maxValue, binaryPos1);
+        }
+        if (minEqual)
+          endIndex = lessEqualPos(arr, minValue, binaryPos2);
+        else {
+          endIndex = lessPos(arr, minValue, binaryPos2);
+        }
+      }
+
+      if (beginIndex > endIndex) {
+        this.hasResult = false;
+        return;
+      }if (beginIndex < 0)
+        beginIndex = 0;
+      else if (endIndex > bound) {
+        endIndex = bound;
+      }
+      this.beginMap.put(v.getName(), Integer.valueOf(beginIndex));
+      this.endMap.put(v.getName(), Integer.valueOf(endIndex));
+    }
+    catch (IOException e)
+    {
+      e.printStackTrace();
+    }
+  }
+
+  public HashMap<String, Integer> getBeginMap() { return this.beginMap; }
+
+  public HashMap<String, Integer> getEndMap() {
+    return this.endMap;
+  }
+}
diff -urN src_origin/contrib/src/java/org/apache/hadoop/hive/contrib/serde2/NcFileSerDe.java src/contrib/src/java/org/apache/hadoop/hive/contrib/serde2/NcFileSerDe.java
--- src_origin/contrib/src/java/org/apache/hadoop/hive/contrib/serde2/NcFileSerDe.java	1970-01-01 08:00:00.000000000 +0800
+++ src/contrib/src/java/org/apache/hadoop/hive/contrib/serde2/NcFileSerDe.java	2013-04-11 00:31:43.993322294 +0800
@@ -0,0 +1,260 @@
+package org.apache.hadoop.hive.contrib.serde2;
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Properties;
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.SerDeException;
+import org.apache.hadoop.hive.serde2.SerDeStats;
+import org.apache.hadoop.hive.serde2.SerDeUtils;
+import org.apache.hadoop.hive.serde2.io.ByteWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.ShortWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
+import org.apache.hadoop.hive.serde2.objectinspector.StructField;
+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
+import org.apache.hadoop.io.BooleanWritable;
+import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+
+public class NcFileSerDe
+  implements SerDe
+{
+  public static final Log LOG = LogFactory.getLog(NcFileSerDe.class.getName());
+  StructObjectInspector rowOI;
+  ArrayList<Object> row;
+  int numColumns;
+  List<TypeInfo> columnTypes;
+  List<String> columnNames;
+  public static List<TypeInfo> colTypes;
+  public static List<String> colNames;
+  boolean[] fieldsSkip;
+  ArrayList<Integer> notSkipIDs;
+  ArrayList<Integer> colArray;
+  int[] posMap;
+  public static ArrayList<Integer> notSkipIDsFromInputFormat;
+  Text outputRowText;
+
+  public NcFileSerDe()
+    throws SerDeException
+  {
+  }
+
+  public void initialize(Configuration job, Properties tbl)
+    throws SerDeException
+  {
+    this.notSkipIDs = notSkipIDsFromInputFormat;
+    String columnNameProperty = tbl.getProperty("columns");
+    String columnTypeProperty = tbl.getProperty("columns.types");
+
+    this.columnNames = Arrays.asList(StringUtils.split(columnNameProperty, ','));
+    this.columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);
+
+    assert (this.columnNames.size() == this.columnTypes.size());
+    this.numColumns = this.columnNames.size();
+
+    if ((this.notSkipIDs == null) || (this.notSkipIDs.size() == 0)) {
+      this.notSkipIDs = new ArrayList();
+      for (int i = 0; i < this.numColumns; i++) {
+        this.notSkipIDs.add(Integer.valueOf(i));
+      }
+    }
+
+    for (int c = 0; c < this.numColumns; c++) {
+      if (((TypeInfo)this.columnTypes.get(c)).getCategory() != ObjectInspector.Category.PRIMITIVE) {
+        throw new SerDeException(new StringBuilder().append(getClass().getName()).append(" only accepts primitive columns, but column[").append(c).append("] named ").append((String)this.columnNames.get(c)).append(" has category ").append(((TypeInfo)this.columnTypes.get(c)).getCategory()).toString());
+      }
+
+    }
+
+    List columnOIs = new ArrayList(this.columnNames.size());
+
+    for (int c = 0; c < this.numColumns; c++) {
+      columnOIs.add(TypeInfoUtils.getStandardWritableObjectInspectorFromTypeInfo((TypeInfo)this.columnTypes.get(c)));
+    }
+
+    this.rowOI = ObjectInspectorFactory.getStandardStructObjectInspector(this.columnNames, columnOIs);
+
+    this.row = new ArrayList(this.numColumns);
+    for (int c = 0; c < this.numColumns; c++) {
+      this.row.add(null);
+    }
+
+    this.outputRowText = new Text();
+    this.fieldsSkip = new boolean[this.numColumns];
+    this.posMap = new int[this.numColumns];
+    this.colArray = new ArrayList();
+    for (int i = 0; i < this.numColumns; i++)
+    {
+      for (int j = 0; j < this.notSkipIDs.size(); j++) {
+        int id = ((Integer)this.notSkipIDs.get(j)).intValue();
+        if (id == i) {
+          this.posMap[i] = j;
+
+          this.colArray.add(Integer.valueOf(i));
+          break;
+        }
+
+      }
+
+    }
+
+    colTypes = this.columnTypes;
+    colNames = this.columnNames;
+  }
+
+  public Object deserialize(Writable field) throws SerDeException
+  {
+    Text t = (Text)field;
+
+    String[] list = StringUtils.split(t.toString(), '\t');
+
+    for (Integer i : this.colArray) {
+      try {
+        this.row.set(i.intValue(), deserializeField((TypeInfo)this.columnTypes.get(i.intValue()), list[this.posMap[i.intValue()]]));
+      }
+      catch (IOException e) {
+        e.printStackTrace();
+      }
+    }
+
+    return this.row;
+  }
+
+  static Object deserializeField(TypeInfo type,
+	      String str) throws IOException {
+	    if (str==null)
+	      return null;
+
+	    switch (type.getCategory()) {
+	    case PRIMITIVE: {
+	      PrimitiveTypeInfo ptype = (PrimitiveTypeInfo) type;
+	      switch (ptype.getPrimitiveCategory()) {
+
+	      case VOID: {
+	        return null;
+	      }
+
+	      case BOOLEAN: {
+	        BooleanWritable r=new BooleanWritable();
+	        r.set(new Boolean(str));
+	        return r;
+	      }
+	      case BYTE: {
+	        ByteWritable r=new ByteWritable();
+	        r.set(new Byte(str));
+	        return r;
+	      }
+	      case SHORT: {
+	        ShortWritable r=new ShortWritable();
+	        r.set(new Short(str));
+	        return r;
+	      }
+	      case INT: {
+	        IntWritable r=new IntWritable();
+	        r.set(new Integer(str));
+	        return r;
+	      }
+	      case LONG: {
+	        LongWritable r = new LongWritable();
+	        r.set(new Long(str));
+	        return r;
+	      }
+	      case FLOAT: {
+	        FloatWritable r = new FloatWritable();
+	        r.set(new Float(str));
+	        return r;
+	      }
+	      case DOUBLE: {
+	        DoubleWritable r = new DoubleWritable();
+	        r.set(new Double(str));
+	        return r;
+	      }
+	      case STRING: {
+	        Text r = new Text(str);
+	        return r;
+	      }
+	      default: {
+	        throw new RuntimeException("Unrecognized type: "
+	            + ptype.getPrimitiveCategory());
+	      }
+	      }
+	    }
+	      // Currently, deserialization of complex types is not supported
+	    case LIST:
+	    case MAP:
+	    case STRUCT:
+	    default: {
+	      throw new RuntimeException("Unsupported category: " + type.getCategory());
+	    }
+	    }
+	  }
+
+
+  public ObjectInspector getObjectInspector()
+    throws SerDeException
+  {
+    return this.rowOI;
+  }
+
+  public Class<? extends Writable> getSerializedClass()
+  {
+    return Text.class;
+  }
+
+  public Writable serialize(Object obj, ObjectInspector objInspector)
+    throws SerDeException
+  {
+    StructObjectInspector outputRowOI = (StructObjectInspector)objInspector;
+    List outputFieldRefs = outputRowOI.getAllStructFieldRefs();
+
+    if (outputFieldRefs.size() != this.numColumns) {
+      throw new SerDeException(new StringBuilder().append("Cannot serialize the object because there are ").append(outputFieldRefs.size()).append(" fields but the output has ").append(this.colArray.size()).append(" columns.").toString());
+    }
+
+    StringBuilder outputRowString = new StringBuilder();
+
+    for (int c = 0; c < this.numColumns; c++) {
+      if (c > 0) {
+        outputRowString.append('\t');
+      }
+
+      Object column = outputRowOI.getStructFieldData(obj, (StructField)outputFieldRefs.get(c));
+
+      if (((StructField)outputFieldRefs.get(c)).getFieldObjectInspector().getCategory() == ObjectInspector.Category.PRIMITIVE)
+      {
+        outputRowString.append(column == null ? "null" : column.toString());
+        System.out.print(new StringBuilder().append(((StructField)outputFieldRefs.get(c)).getFieldObjectInspector().getTypeName()).append(":").append(column.toString()).append("\t").toString());
+      }
+      else
+      {
+        outputRowString.append(SerDeUtils.getJSONString(column, ((StructField)outputFieldRefs.get(c)).getFieldObjectInspector()));
+      }
+
+    }
+
+    this.outputRowText.set(outputRowString.toString());
+    return this.outputRowText;
+  }
+
+  public SerDeStats getSerDeStats() {
+    // no support for statistics
+    return null;
+  }
+}
diff -urN src_origin/ivy/libraries.properties src/ivy/libraries.properties
--- src_origin/ivy/libraries.properties	2013-04-12 12:31:20.283528416 +0800
+++ src/ivy/libraries.properties	2013-04-10 22:08:31.820830777 +0800
@@ -44,7 +44,8 @@
 derby.version=10.4.2.0
 guava-hadoop20.version=r09
 guava-hadoop23.version=11.0.2
-hbase.version=0.92.0
+#hbase.version=0.92.0
+hbase.version=0.94.6.1
 jackson.version=1.8.8
 javaewah.version=0.3.2
 jdo-api.version=2.3-ec
Binary files src_origin/lib/netcdf-4.3.16-SNAPSHOT.jar and src/lib/netcdf-4.3.16-SNAPSHOT.jar differ
diff -urN src_origin/ql/ivy.xml src/ql/ivy.xml
--- src_origin/ql/ivy.xml	2013-04-12 12:31:20.075528408 +0800
+++ src/ql/ivy.xml	2013-04-10 15:08:51.329821998 +0800
@@ -42,6 +42,8 @@
     <!-- hadoop specific guava -->
     <dependency org="com.google.guava" name="guava" rev="${guava-hadoop20.version}"
                 conf="hadoop20.compile->default" transitive="false"/>
+<!--    <dependency org="com.google.guava" name="guava" rev="${guava-hadoop20S.version}"-->
+<!--                conf="hadoop20S.compile->default" transitive="false"/>-->
     <dependency org="com.google.guava" name="guava" rev="${guava-hadoop23.version}"
                 conf="hadoop23.compile->default" transitive="false"/>
 
diff -urN src_origin/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java src/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
--- src_origin/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java	2013-04-12 12:31:20.087528407 +0800
+++ src/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java	2013-04-10 13:19:03.701503175 +0800
@@ -199,6 +199,18 @@
     }
   }
 
+  private static Map<String, LinkedHashMap<String, ArrayList<ExprNodeDesc>>>pathToNodeDescMap;
+  public static void setPathToNodeDesc(String jobID,LinkedHashMap<String, ArrayList<ExprNodeDesc>> pathToNodeDesc){
+	  if(pathToNodeDescMap==null){
+		  pathToNodeDescMap = Collections
+			      .synchronizedMap(new HashMap<String,LinkedHashMap<String, ArrayList<ExprNodeDesc>>>());
+	  }
+	  pathToNodeDescMap.put(jobID, pathToNodeDesc);
+  }
+  public static LinkedHashMap<String, ArrayList<ExprNodeDesc>> getPathToNodeDesc(String jobID) {
+     return pathToNodeDescMap.get(jobID);
+  }
+  
   public static MapredWork getMapRedWork(Configuration job) {
     MapredWork gWork = null;
     try {
@@ -218,6 +230,7 @@
         MapredWork ret = deserializeMapRedWork(in, job);
         gWork = ret;
         gWork.initialize();
+        setPathToNodeDesc(jobID,gWork.getPathToNodeDesc());
         gWorkMap.put(jobID, gWork);
       }
       return (gWork);
@@ -503,6 +516,7 @@
       // workaround for java 1.5
       e.setPersistenceDelegate(ExpressionTypes.class, new EnumDelegate());
       e.setPersistenceDelegate(GroupByDesc.Mode.class, new EnumDelegate());
+      w.computePredicate();
       e.writeObject(w);
     } finally {
       if (null != e) {
diff -urN src_origin/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java src/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java
--- src_origin/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java	2013-04-12 12:31:20.083528407 +0800
+++ src/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java	2013-04-10 21:58:06.172800497 +0800
@@ -30,6 +30,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.exec.FilterOperator;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.parse.OpParseContext;
 import org.apache.hadoop.hive.ql.parse.QBJoinTree;
@@ -93,7 +94,90 @@
   private boolean inputFormatSorted = false;
 
   private transient boolean useBucketizedHiveInputFormat;
+    
+    private LinkedHashMap<String, ArrayList<ExprNodeDesc>> pathToNodeDesc;
 
+	public void putPathToNodeDesc(String path, ExprNodeDesc expr) {
+		if (this.pathToNodeDesc == null) {
+			this.pathToNodeDesc = new LinkedHashMap<String, ArrayList<ExprNodeDesc>>();
+		}
+		if (this.pathToNodeDesc.containsKey(path)) {
+			((ArrayList<ExprNodeDesc>) this.pathToNodeDesc.get(path)).add(expr);
+		} else {
+			ArrayList<ExprNodeDesc> lst = new ArrayList<ExprNodeDesc>();
+			lst.add(expr);
+			this.pathToNodeDesc.put(path, lst);
+		}
+	}
+
+	public LinkedHashMap<String, ArrayList<ExprNodeDesc>> getPathToNodeDesc() {
+		return this.pathToNodeDesc;
+	}
+
+	public void setPathToNodeDesc(
+			LinkedHashMap<String, ArrayList<ExprNodeDesc>> pathToNodeDesc) {
+		this.pathToNodeDesc = pathToNodeDesc;
+	}
+
+	public void computePredicate() {
+		String key;
+		if ((this.pathToNodeDesc == null) && (this.pathToAliases != null))
+			for (Iterator i = this.pathToAliases.keySet().iterator(); i
+					.hasNext();) {
+				key = (String) i.next();
+				for (String alias : (ArrayList<String>) this.pathToAliases
+						.get(key))
+					addPredicate((Operator) this.aliasToWork.get(alias), key);
+			}
+
+	}
+
+	public void addPredicate(Operator<? extends OperatorDesc> op, String key) {
+		if (op instanceof FilterOperator) {
+			putPathToNodeDesc(key,
+					((FilterDesc) ((FilterOperator) op).getConf())
+							.getPredicate());
+			return;
+		}
+		if (op.getChildOperators() != null)
+			for (Operator cop : op.getChildOperators())
+				addPredicate(cop, key);
+	}
+
+	public void dispalyMapOperatorTree() {
+		System.out.println("Map Operator");
+		if ((this.aliasToWork != null) && (this.aliasToWork.size() != 0))
+			for (String key : this.aliasToWork.keySet())
+				displayOperatorTree((Operator) this.aliasToWork.get(key), 0);
+	}
+
+	public void displayReduceOperatorTree() {
+		System.out.println("Reduce Operator");
+		if (this.reducer != null)
+			displayOperatorTree(this.reducer, 0);
+	}
+
+	public void displayOperatorTree(Operator<? extends OperatorDesc> op,
+			int indent) {
+		System.out.print(indentString(indent));
+		System.out.println(op.getType());
+		if ((op instanceof FilterOperator)) {
+			System.out.print(indentString(indent));
+			System.out.println(((FilterDesc) ((FilterOperator) op).getConf())
+					.getPredicate().getExprString());
+		}
+		if (op.getChildOperators() != null)
+			for (Operator cop : op.getChildOperators())
+				displayOperatorTree(cop, indent + 2);
+	}
+
+	private String indentString(int indent) {
+		StringBuilder sb = new StringBuilder();
+		for (int i = 0; i < indent; i++) {
+			sb.append(" ");
+		}
+		return sb.toString();
+	}
   public MapredWork() {
     aliasToPartnInfo = new LinkedHashMap<String, PartitionDesc>();
   }
diff -urN src_origin/shims/ivy.xml src/shims/ivy.xml
--- src_origin/shims/ivy.xml	2013-04-12 12:31:20.367528420 +0800
+++ src/shims/ivy.xml	2013-04-10 15:00:33.885797923 +0800
@@ -124,6 +124,8 @@
       <include type="jar"/>
       <exclude org="commons-daemon" module="commons-daemon"/><!--bad POM-->
       <exclude org="org.apache.commons" module="commons-daemon"/><!--bad POM-->
+      <exclude org="org.codehaus.jackson" name="jackson-core-asl"/><!--bad POM-->
+      <exclude org="org.codehaus.jackson" name="jackson-mapper-asl"/><!--bad POM-->
     </dependency>
     <dependency org="org.apache.hadoop" name="hadoop-tools"
                 rev="${hadoop-0.20S.version}"
@@ -131,6 +133,8 @@
       <include type="jar"/>
       <exclude org="commons-daemon" module="commons-daemon"/><!--bad POM-->
       <exclude org="org.apache.commons" module="commons-daemon"/><!--bad POM-->
+      <exclude org="org.codehaus.jackson" name="jackson-core-asl"/><!--bad POM-->
+      <exclude org="org.codehaus.jackson" name="jackson-mapper-asl"/><!--bad POM-->
     </dependency>
     <dependency org="org.apache.hadoop" name="hadoop-test"
                 rev="${hadoop-0.20S.version}"
@@ -138,6 +142,8 @@
       <include type="jar"/>
       <exclude org="commons-daemon" module="commons-daemon"/><!--bad POM-->
       <exclude org="org.apache.commons" module="commons-daemon"/><!--bad POM-->
+      <exclude org="org.codehaus.jackson" name="jackson-core-asl"/><!--bad POM-->
+      <exclude org="org.codehaus.jackson" name="jackson-mapper-asl"/><!--bad POM-->
     </dependency>
 
     <!-- Test Dependencies -->
